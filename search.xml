<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[维纳和香农的信息熵|读维纳《控制论》]]></title>
    <url>%2F2018%2F07%2F15%2F%E6%8E%A7%E5%88%B6%E8%AE%BA1%E5%85%B3%E4%BA%8E%E4%BF%A1%E6%81%AF%E7%86%B5%2F</url>
    <content type="text"><![CDATA[对信息进行数学上的量化是个非常了不起的想法。 维纳的控制论和香农的信息论几乎不分先后地分别独立提出了两个近似的概念，即信息量和信息熵，它们的数学形式相近，只相差一个负号。维纳的《控制论（或关于在动物和机器中控制和通信的科学）》中对其信息量的推导做了一定的说明，但其语言较简练，翻译也不是非常易懂，这里尝试按自己的理解对其推导进行整理，并整理了香农的信息熵的推导，以作为对照和参考。 维纳的信息量首先，一个最基础的事件由两种选择构成，即「是」和「否」。一个最简单的最基本的信息形式，就是对两个具有相同概率的二中择一的简单事件所作选择的记录。这就如同数学上用二分法来确定一个数：首先确定数是否比 $ \frac12 $ 大，这是第一次选择；然后确定是否比 $ \frac34 $ 大，这是第二次选择……依次类推，除了某些特殊情况，这项先二分再判断的工作可以无限进行下去， 则所做的选择次数为无限次，因而所求的信息量为无限大。 接下来尝试用数学公式来表示信息量，前文维纳已经提到了，参考贝尔实验室的研究成果，对于计算机而言，相较于十进制，二进制是效率更高的选择；另外二进制在最简单的事件（即对二中择一的事件）记录中显然比十进制更加简便。考虑一个二进制小数的小数部分，$$ 0.a_1a_2a_3…a_n… $$ 其中 $a_n$ 分别代表该二进制小数的第n位小数，其值要么为 1，要么为 0。则该二进制小数对应的十进制小数的小数部分 $ a $ 可表示为，$$ a=0.a_1a_2a_3…a_n…=\frac12\cdot a_1+\frac1{2^2}\cdot a_2+…+\frac1{2^n}\cdot a_n+… $$这可以理解为利用了二分法的思想，每取一位小数就进行了一次「是」(1)或「否」(0)的判断，如同取一把 [0,1] 的计算尺上的任何一个数，理论上我们都能用不断二等分的方法无限趋近。其中取等分点的次数即是 n。也就是说对于 a 这个数，其包含的最基础的二元判断次数是 n。 但实际上，如同由于现实中误差是一定存在的，我们也不可能要求一个信息无限精确。那么假设计算尺的平均误差用二进制表示为，$$ b=0.b_1b_2b_3…b_k…b_n… $$ 其中 $b_n$ 分别代表该二进制小数的第n位小数，其值要么为 1，要么为 0； 而 $b_k$ 的值为 1， $b_k$ 之前的所有值都为 0。则误差 $b$ 可以写成$$b=0.000…01b_{k+1}…b_n…=\frac12\cdot 0+\frac1{2^2}\cdot 0+…+\frac1{2^k}\cdot 1+…+\frac1{2^n}\cdot b_n+… $$也就是说，误差该计算尺最多能进行的判断次数是 (k-1)，如果大于 (k-1)，其最后一位精确小数已经大于误差，没有实际意义。而$$-\log_2b&gt;-\log_2(2\times0.b_1b_2b_3…b_k)=k-1$$$$-\log_2b\le-\log_20.b_1b_2b_3…b_k=k$$所以我们实际上可以把一把计算尺的最大判断次数记为，$$-\log_2b$$纳维在书中写道， $$-\log_2b_1b_2b_3…b_n，$$我们将用这个量作为信息量的精确公式，同时就把它作为信息量的定义。 以上就是纳维推导出的信息量的数学表达式，即事件的信息量取决与其不确定性的大小，不确定性越大，则所包含的信息量越小。那么不确定性的大小如何确定？按照上述推导可见，式子中的不确定性 b 是在区间 [0,1] 上的误差，所以如维纳写道的， 我们可以如下地来考虑这个定义：事前已知一变数落在 0 到 1 之间，事后得知它落在 (0,1) 中的区间 (a,b) 上。于是我们从事后知识中得到的信息量为$$-\log_2\frac{(a,b)的测度}{(0,1)的测度}$$ 接下来，考虑这样一种情形，一个事件最后会落在 x 轴上，且在 x 轴上随着 x 的变化其概率发生变化，且概率密度函数为 $f_{(x)}$，则可知事件最后落在 (x,x+dx) 区间上的概率为 $f_{(x)}dx$，区间上的信息量则为，$$-\log_2\frac{f_{(x)}dx}{\int_{ - \infty }^{ + \infty }f_{(x)}dx}$$而 $f_{(x)}$ 为概率密度，即整个 x 轴上的上的概率为，$$\int_{ - \infty }^{ + \infty }f_{(x)}dx=1$$相应的，信息量的合理测度为，$$-\int_{ - \infty }^{ + \infty }[\log_2f_{(x)}dx]f_{(x)}dx=-\int_{ - \infty }^{ + \infty }[\log_2f_{(x)}]f_{(x)}dx-\int_{ - \infty }^{ + \infty }f_{(x)}\log_2(dx)dx$$其中 $\log_2(dx)dx$为二阶小量，可以舍去。故信息量可写作，$$-\int_{ - \infty }^{ + \infty }[\log_2f_{(x)}]f_{(x)}dx$$这便是推导出来维纳的信息量的定义，但实际上维纳本人推导出的信息量定义缺少了一个负号，原文中如此写到： 然后，让我们现在来考虑另外一种情形：我们事前的知识是已知某个量应落在 x 到 x+dx 之间的概率为 $f_{1(x)}dx$，事后的知识是得知这概率为 $f_{2(x)}dx$，试问，事后的知识给了我们多少新的信息？ 这个问题的实质上是把曲线 $y=f_{1(x)}dx$ 和 $y=f_{2(x)}dx$ 下的区域的大小用某种宽度来表示。应当注意，我们这里要假定变数 x 具有基本均匀分布，就是说，如果用 $x^3$ 或任何 x 的其他函数来代替 x，我们的结果一般不会相同。由于 $f_{1(x)}dx$ 是概率密度，我们有$$\int_{ - \infty }^{ + \infty }f_{1(x)}dx=1$$ 因而，$f_{1(x)}dx$ 下区域的宽度的平均对数可以看成 $f_{1(x)}dx$ 倒数的对数之高度的某种平均。因此，相应于曲线 $f_{1(x)}dx$ 的信息量的合理测度为$$\int_{ - \infty }^{ + \infty }[\log_2f_{1(x)}]f_{(x)}dx$$这个我们把它定义为信息量的量，是通常在类似情况下定义为熵的那个量的负数。 这便是维纳给出的信息量的量的定义。可见维纳本人也注意到了他的信息量与香农的信息熵（考虑到香农的信息熵理论的提出几乎于维纳同时，可能此处维纳说的熵是指吉布斯的热力学意义上的熵）存在符号上的差异，但就推导过程来看，个人觉得维纳在推导的中途丢掉了信息量的负号是没有理由的。 香农的信息熵香农的信息论是关于通讯信息理论，而通讯的目的在于消除不确定性，也就是说从发出端到接收端发出的信息消除了接收端对于某个事件的不确定性，这种消除的不确定性的量就是香农推导出的信息熵。 在香农1948年的论文A Mathematical Theory of Communication中，他直接给出了信息熵的定义:$$H=-\sum p_i\log_2p_i$$接着，他通过论证信息熵函数需要满足的多条性质，推导出信息熵公式的唯一性。先考虑简单事件的信息熵，一个简单事件只有发生与不发生，假定接收端知道某事件发生概率为 $p_i$（香农构想的通讯过程中, 发送端发送消息的基元数和每一消息的发送概率, 都是预先规定好了的, 而接收端对这一规定又是完全明了的）则发出端发出的一条告知接收端该事件发生了的信息所包含的信息量应有如下性质： 信息量的大小是关于 $p_i$ 的单调连续函数。 直观上理解，假如有人告诉你太阳从东边升起，那么这条信息的信息量显然为零，因为你知道只要人站在地面上观察，太阳一定是从东边升起；假如有人告诉你刚刚抛硬币的结果是正面朝上，那么这条消息是有一定价值的，因为它排除的一半的不确定性。所以，事件的概率显然关系到传递的消息的信息量，且发生的概率越低，消息消除的不确定性越大，消息包含的信息量越大。 $h_{(x+y)}=h_{(x)}+h_{(y)}$. 考虑某些事件的达成实际上可能依赖两个事件的达成，比如「两次投硬币都是正面」这一事件就是「第一次投硬币结果是正面」和「第二次投硬币结果是正面」这两个事件的叠加的结果，所以很自然地，分别传递这两次投掷硬币的结果和单独传递两次投硬币后的总结果的两种信息量应是相同的，且显然两次投硬币为正的概率是每次投硬币为正的概率的积。则若，$$h_{(x)}=f_{(p_{x})}$$则有，$$h_{(x)}=f_{(p_{x}\cdot p_{y})}=f_{p_x}+f_{p_y}$$ 综合上述考虑，信息量的形式必然只能为 $f_{(p)}=\log_ap$。对该式，香农做了一点小变化： 考虑到信息量为正数比较符合直观感受（即事件概率越大，传递信息排除的不确定性越小，信息量越小），给上式添加一个负号； 对数函数的底 a 可以是任何数，但香农选择了 2，这是人为确定的，但这个选择是很有道理的，至少从后来计算机的发展中我们也可以看出，二进制在电子计算机方面是有巨大优势的，以 2 为底是比以 e 为底更加方便简洁的。 所以我们得出了单个的简单事件的信息量为，$$h(p)=-\log_2p$$ 而一个事件可能有多种结果，每个结果有各自的概率，那么对该系统而言，我们可以用熵来衡量整个系统的平均信息量，这种平均也就是加权平均，即信息量的期望值，所以我们就得到了香农的信息熵公式：$$H=-\sum p_i\log_2p_i$$]]></content>
      <tags>
        <tag>信息熵</tag>
        <tag>控制论</tag>
      </tags>
  </entry>
</search>
